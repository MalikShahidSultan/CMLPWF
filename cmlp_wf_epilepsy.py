# -*- coding: utf-8 -*-
"""cmlp_wf_epilepsy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HdtT6if6GMrtrMqX3paQDP7NjK9HTvER

Import Libraries and Helper Function
"""

from google.colab import drive
drive.mount('/content/drive')

device = torch.device('cuda')

import os
import pandas as pd
import torch
import torch.nn as nn
import numpy as np
from torch.linalg import norm

import sys
import os
import json
import torch
import numpy as np
import time

class TrainableEltWiseLayer(nn.Module):

  def __init__(self, shape):
    
    super(TrainableEltWiseLayer, self).__init__()
    self.weight = nn.Parameter(torch.ones(shape))
    self.reset_parameters()

  def reset_parameters(self) -> None:

  # nn.init.constant_(self.weight.data, 0.)
    nn.init.constant_(self.weight.data, 1.)

  def forward(self, x):

  # return x * torch.sigmoid(self.weight)
    return x * self.weight


def activation_helper(activation):
  if activation == 'sigmoid':
      act = nn.Sigmoid()
  elif activation == 'tanh':
      act = nn.Tanh()
  elif activation == 'relu':
      act = nn.ReLU()
  elif activation == 'leakyrelu':
      act = nn.LeakyReLU()
  elif activation is None:
      def act(x):
          return x
  else:
      raise ValueError('unsupported activation: %s' % activation)
  return act


class CMLP(nn.Module):

  def __init__(self, input_size=10, seq_len=10, hidden_dim=10, n_layers=2, output_size=1, act='sigmoid'):

    super(CMLP, self).__init__()

    self.n_layers = n_layers
    self.seq_len = seq_len
    self.input_size = input_size

    # Linear Layer Input
    self.linear = nn.Linear(input_size * seq_len, hidden_dim)

    # Linear layers
    self.hidden_s = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for _ in range(n_layers - 1)])

    # Linear Layer
    self.fc = nn.Linear(hidden_dim, output_size)

    # activation
    self.act = activation_helper(act)

  def forward(self, input):
      out = self.act(self.linear(input))
      for layer in self.hidden_s:
          out = self.act(layer(out))
      out = self.fc(out)
      return out

  def regularize(self, lam):
      # Group Lasso on features
      reg_features = lam * norm(self.linear.weight.reshape(-1, self.input_size), dim=(0,)).sum()
      # Group Lasso on time steps
      reg_lags = lam * norm(self.linear.weight.T.reshape(self.seq_len, -1), dim=(1,)).sum()
      return reg_features + reg_lags

  @torch.no_grad()
  def prox(self, lam):
      # Group Lasso on features
      shape = self.linear.weight.shape
      self.linear.weight.data = (nn.ReLU()(
          1 - lam / norm(self.linear.weight.reshape(-1, self.input_size), dim=(0,), keepdim=True)) *
          self.linear.weight.reshape(-1, self.input_size)).reshape(shape)
      # Group Lasso on time steps
      shape_t = self.linear.weight.T.shape
      self.linear.weight.data = (nn.ReLU()(
          1 - lam / norm(self.linear.weight.T.reshape(self.seq_len, -1), dim=(1,), keepdim=True)) *
          self.linear.weight.T.reshape(self.seq_len, -1)).reshape(shape_t).T

  @torch.no_grad()
  def GC(self, threshold=False, ignore_lag=True):
      if ignore_lag:
          GC = norm(self.linear.weight.reshape(-1, self.input_size), dim=(0,))
      else:
          GC = norm(self.linear.weight, dim=(0,))
      if threshold:
          return (GC > 0).int().cpu().numpy()
      else:
          return GC.cpu().numpy()


class CMLPFull(nn.Module):

  def __init__(self, input_size=10, seq_len=10, hidden_dim=10, n_layers=2, act='sigmoid'):
      super(CMLPFull, self).__init__()
      self.seq_len = seq_len
      self.input_size = input_size
      self.networks = nn.ModuleList([
          CMLP(input_size=input_size, seq_len=seq_len, hidden_dim=hidden_dim,
                n_layers=n_layers, act=act)
          for _ in range(input_size)])

  def forward(self, input):
      input_flatten = input.reshape(-1, self.seq_len * self.input_size)
      out = torch.cat([network(input_flatten) for network in self.networks], dim=1)
      return out

  def regularize(self, lam):
      reg_loss = 0
      for net in self.networks:
          reg_loss += net.regularize(lam)
      return reg_loss

  @torch.no_grad()
  def prox(self, lam):
      for net in self.networks:
          net.prox(lam)

  @torch.no_grad()
  def GC(self, threshold=False, ignore_lag=True):
      return np.stack([net.GC(threshold, ignore_lag) for net in self.networks])


class CMLPwFilter(nn.Module):

  def __init__(self, input_size=10, seq_len=10, hidden_dim=10, n_layers=2, output_size=1, act='sigmoid'):
      super(CMLPwFilter, self).__init__()

      self.n_layers = n_layers
      self.seq_len = seq_len
      self.input_size = input_size

      # Filters
      self.filter_features = TrainableEltWiseLayer(input_size)
      self.filter_lags = TrainableEltWiseLayer((seq_len, 1))

      # Linear Layer Input
      self.linear = nn.Linear(input_size * seq_len, hidden_dim)

      # LSTM layers
      self.hidden_s = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for _ in range(n_layers - 1)])

      # Linear Layer
      self.fc = nn.Linear(hidden_dim, output_size)

      # activation
      self.act = activation_helper(act)

      # linear layer norm
      with torch.no_grad():
          self.norm_const = norm(self.linear.weight).detach()
          

  def forward(self, input):
      input = input.float()
      filtered_input = self.filter_lags(self.filter_features(input))
      # normalized weight for linear
      normalized_input = self.norm_const / norm(self.linear.weight) * filtered_input
      flatten_input = normalized_input.reshape(-1, self.seq_len * self.input_size)
      out = self.act(self.linear(flatten_input))
      for layer in self.hidden_s:
          out = self.act(layer(out))
      out = self.fc(out)
      return out

  def regularize(self, lam):
      # Group Lasso on features
      # reg_features = lam * norm(self.filter.weight.reshape(-1, self.input_size), dim=(0,)).sum()
      reg_features = lam * torch.abs(self.filter_features.weight).sum()
      # Group Lasso on time steps
      # reg_lags = lam * norm(self.filter.weight.T.reshape(self.seq_len, -1), dim=(1,)).sum()
      reg_lags = lam * torch.abs(self.filter_lags.weight).sum()
      return reg_features + reg_lags

  @torch.no_grad()
  def prox(self, lam):
      # Group Lasso on features
      # shape = self.filter.weight.shape
      # self.filter.weight.data = (nn.ReLU()(
      #     1 - lam / norm(self.filter.weight.reshape(-1, self.input_size), dim=(0,), keepdim=True)) *
      #                            self.filter.weight.reshape(-1, self.input_size)).reshape(shape)
      self.filter_features.weight.data = nn.ReLU()(
          1 - lam / torch.abs(self.filter_features.weight)) * self.filter_features.weight
      # Group Lasso on time steps
      # shape_t = self.filter.weight.T.shape
      # self.filter.weight.data = (nn.ReLU()(
      #     1 - lam / norm(self.filter.weight.T.reshape(self.seq_len, -1), dim=(1,), keepdim=True)) *
      #                            self.filter.weight.T.reshape(self.seq_len, -1)).reshape(shape_t).T
      self.filter_lags.weight.data = nn.ReLU()(
          1 - lam / torch.abs(self.filter_lags.weight)) * self.filter_lags.weight

  @torch.no_grad()
  def GC(self, threshold=False, ignore_lag=True):
      if ignore_lag:
          # GC = norm(self.filter.weight.reshape(-1, self.input_size), dim=(0,))
          GC = torch.abs(self.filter_features.weight)
      else:
          # GC = norm(self.filter.weight, dim=(0,))
          GC = torch.abs(self.filter_lags.weight)
      if threshold:
          return (GC > 0).int().cpu().numpy()
      else:
          return GC.cpu().numpy()


class CMLPwFilterFull(nn.Module):

  def __init__(self, input_size=10, seq_len=10, hidden_dim=20, n_layers=5, act='sigmoid'):
      super(CMLPwFilterFull, self).__init__()
      self.seq_len = seq_len
      self.input_size = input_size
      self.networks = nn.ModuleList([
          CMLPwFilter(input_size=input_size, seq_len=seq_len, hidden_dim=hidden_dim,
                      n_layers=n_layers, act=act)
          for _ in range(input_size)])

  def forward(self, input):
      out = torch.cat([network(input) for network in self.networks], dim=1)
      return out

  def regularize(self, lam):
      reg_loss = 0
      for net in self.networks:
          reg_loss += net.regularize(lam)
      return reg_loss

  @torch.no_grad()
  def prox(self, lam):
      for net in self.networks:
          net.prox(lam)

  @torch.no_grad()
  def GC(self, threshold=False, ignore_lag=True):
      return np.stack([net.GC(threshold, ignore_lag) for net in self.networks])

class LeKVAR(nn.Module):

    def __init__(self, input_size=10, seq_len=10, hidden_dim=10, n_layers=2, act='sigmoid', mode=None):
        super(LeKVAR, self).__init__()

        self.n_layers = n_layers
        self.seq_len = seq_len
        self.input_size = input_size
        self.hidden_dim = hidden_dim
        self.mode = mode

        if mode == "nn":
            # Linear Layer Input
            self.linear = nn.Linear(1, hidden_dim)

            # Middle layers
            self.hidden_s = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for _ in range(n_layers - 1)])

            # Last Layer
            # Could be extended to different output size (>1)
            self.last = nn.Linear(hidden_dim, 1)

            # linear layer norm
            with torch.no_grad():
                self.norm_const = norm(self.last.weight).detach()

        # Linear Layer
        self.fc = nn.Linear(input_size * seq_len, input_size)

        # activation
        self.act = activation_helper(act)

    def kernel_forward(self, input):
        if self.mode is None:
            return input

        if self.mode == 'nn':
            shape = input.shape
            single_input = input.reshape((*shape, 1))
            out = self.act(self.linear(single_input))
            for layer in self.hidden_s:
                out = self.act(layer(out))
            out = self.norm_const / norm(self.last.weight) * out
            out = self.last(out)
            return out.reshape(shape)

        if self.mode == 'sq':
            return -2 + 1/2 * input**2

    def forward(self, input):
        out = self.kernel_forward(input)
        out = self.fc(out.reshape(-1, self.seq_len * self.input_size))
        return out

    def regularize(self, lam):
        # Group Lasso on features
        reg_features = lam * \
            norm(self.fc.weight.reshape(self.input_size, -1, self.input_size), dim=(1,)).sum()
        # Group Lasso on time steps
        reg_lags = lam * \
            norm(self.fc.weight.T.reshape(self.seq_len, -1, self.input_size), dim=(1,)).sum()
      
        return reg_features + reg_lags

    @torch.no_grad()
    def prox(self, lam):
        # Group Lasso prox on features
        shape = self.fc.weight.shape
        self.fc.weight.data = (nn.ReLU()(
           1 - lam / norm(self.fc.weight.reshape(self.input_size, -1, self.input_size), dim=(1,), keepdim=True)) *
           self.fc.weight.reshape(self.input_size, -1, self.input_size)).reshape(shape)
        # Group Lasso prox time steps
        shape_t = self.fc.weight.T.shape
        self.fc.weight.data = (nn.ReLU()(
            1 - lam / norm(self.fc.weight.T.reshape(self.seq_len, -1, self.input_size), dim=(1,), keepdim=True)) *
            self.fc.weight.T.reshape(self.seq_len, -1, self.input_size)).reshape(shape_t).T

    @torch.no_grad()
    def GC(self, threshold=False, ignore_lag=True):
        if ignore_lag:
            GC = norm(self.fc.weight.reshape(self.input_size, -1, self.input_size), dim=(1,))
        else:
            GC = torch.abs(self.fc.weight)
            GC = torch.stack([GC[:, self.input_size*i:self.input_size*(i+1)] for i in range(self.seq_len)])
        if threshold:
            return (GC > 0).int().cpu().numpy()
        else:
            return GC.cpu().numpy()


def cmlp(input_size=10, seq_len=10):
  return CMLPFull(input_size, seq_len)


def cmlpwf(input_size=10, seq_len=10):
  return CMLPwFilterFull(input_size, seq_len)


def var(input_size=10, seq_len=10):
  return LeKVAR(input_size, seq_len, mode=None)


def lekvar(input_size=10, seq_len=10):
  return LeKVAR(input_size, seq_len, mode='nn')

def cmlp_s(input_size=10, seq_len=10):
  return CMLPFull(input_size, seq_len, n_layers=1)


def cmlpwf_s(input_size=10, seq_len=10):
  return CMLPwFilterFull(input_size, seq_len, n_layers=1)


def sqvar(input_size=10, seq_len=10):
  return LeKVAR(input_size, seq_len, mode='sq')

from torch.utils.data import Dataset,DataLoader
class PrepareData(Dataset):

    def __init__(self, X, y, scale_X=True):
        if not torch.is_tensor(X):

            self.X = torch.from_numpy(X)
        if not torch.is_tensor(y):
            self.y = torch.from_numpy(y)
        else:
          self.X = X
          self.y = y

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

"""Model and Details"""

from torch.nn.modules import loss


def initialise_model(model_name, input_size, seq_len, resume_from=None, load_best=None):

    model = getattr(models, model_name)(input_size, seq_len)

    current_round = 0
    if resume_from:
        model, current_round = load_checkpoint(resume_from, load_best)

    return model, current_round


def get_training_elements(model_name, input_size, seq_len, resume_from, load_best, gpu):
    # Define the model
    model, current_round = initialise_model(
        model_name, input_size, seq_len, resume_from, load_best)

    model = model_to_device(model, gpu)

    criterion = nn.MSELoss()

    return model, criterion, current_round


from torch.optim.lr_scheduler import MultiStepLR, CosineAnnealingLR
from torch.optim import SGD, Adam, RMSprop

def get_optimiser(params_to_update, optimiser_name, lr, momentum, weight_decay):
    if optimiser_name == 'sgd':
        optimiser = SGD(params_to_update, lr,
                        momentum=momentum,
                        weight_decay=weight_decay)
    elif optimiser_name == 'adam':
        optimiser = Adam(params_to_update, lr)
    elif optimiser_name == 'rmsprop':
        optimiser = RMSprop(params_to_update, lr,
                            momentum=momentum,
                            weight_decay=weight_decay)
    else:
        raise ValueError("optimiser not supported")

    return optimiser
@torch.no_grad()
def prox_step(model, scheduler, lam):
    #model.prox(lam * scheduler.get_last_lr()[0])
    model.prox(lam)

def get_lr_scheduler(optimiser, total_epochs, method='static'):
    """
    Implement learning rate scheduler.
    :param optimiser: A reference to the optimiser being used
    :param total_epochs: The total number of epochs (from the args)
    :param method: The strategy to adjust the learning rate
    (multistep, cosine or static)
    :returns: scheduler on current step/epoch/policy
    """
    if method == 'cosine':
        return CosineAnnealingLR(optimiser, total_epochs)
    elif method == 'static':
        return MultiStepLR(optimiser, [total_epochs + 1])
    if method == 'cifar_1':
        return MultiStepLR(optimiser, [int(0.5 * total_epochs),
                                       int(0.75 * total_epochs)], gamma=0.1)
    if method == 'cifar_2':
        return MultiStepLR(optimiser, [int(0.3 * total_epochs),
                                       int(0.6 * total_epochs), int(0.8 * total_epochs)],
                           gamma=0.2)
    raise ValueError(f"{method} is not defined as scheduler name.")



import numpy as np
import os
import json
import glob


class AverageMeter(object):
    """Computes and stores the average and current value"""
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count if self.count > 0 else None

    def get_val(self):
        return self.val

    def get_avg(self):
        return self.avg


def init_metrics_meter(epoch=None):
    if round is not None:
        metrics_meter = {
            'round': epoch,
            'loss': AverageMeter(),
            'GC_full': 0.
        }
    else:
        metrics_meter = {
            'train_round': [], 'train_loss': [],
            'train_GC_full': [],
            'test_round': [], 'test_loss': [],
            'test_GC_full': []
        }
    return metrics_meter


def get_model_str_from_obj(model):
    return str(list(model.modules())[0]).split("\n")[0][:-1]


def create_metrics_dict(metrics):
    metrics_dict = {'round': metrics['round']}
    for k in metrics:
        if k == 'round':
            continue
        if isinstance(metrics[k], AverageMeter):
            metrics_dict[k] = metrics[k].get_avg()
        else:
            metrics_dict[k] = metrics[k]
    return metrics_dict


def create_model_dir(args, lr=True):
    model_dataset = '_'.join([args.model, args.dataset])
    run_id = f'id={args.run_id}'

    model_dir = os.path.join(
        args.checkpoint_dir, model_dataset, run_id)
    if lr:
        run_hp = os.path.join(
            f"lr={str(args.lr)}-gc_pen={str(args.gc_penalty)}",
            f"seed={str(args.manual_seed)}")
        model_dir = os.path.join(model_dir, run_hp)

    return model_dir


def log_epoch_info(Logger, i, total_iters, metrics_meter, dataload_duration,
                   inference_duration, backprop_duration, train=True):
    mode_str = 'Train' if train else 'Test'
    Logger.get().info("{mode_str} [{round}][{current_batch}/{total_batches}]\t"
                      "DataLoad time {dataload_duration:.3f}\t"
                      "F/W time {inference_duration:.3f}\t"
                      "B/W time {backprop_duration:.3f}\t"
                      "Loss {loss:.4f}\t".format(
                        mode_str=mode_str,
                        round=metrics_meter['round'],
                        current_batch=i,
                        total_batches=total_iters,
                        dataload_duration=dataload_duration,
                        inference_duration=inference_duration,
                        backprop_duration=backprop_duration,
                        loss=metrics_meter['loss'].get_avg()))


def metric_to_dict(metrics_meter, round, preffix='', all_prefix=True):
    round_preffix = preffix + '_round' if all_prefix else 'round'
    out = {
        round_preffix: round,
        preffix + '_loss': metrics_meter['loss'].get_avg(),
        preffix + '_roc_auc': metrics_meter['roc_auc'],
        preffix + '_pr_auc': metrics_meter['pr_auc'],
        preffix + '_GC': metrics_meter['GC'],
        preffix + '_GC_full': metrics_meter['GC_full'],
    }
    return out


def extend_metrics_dict(full_metrics, last_metrics):
    for k in last_metrics:
        if last_metrics[k] is not None:
            full_metrics[k].append(last_metrics[k])


def get_key(train=True):
    return 'train_' if train else 'test_'


def get_best_lr_and_metric(args, last=False):
    best_arg, best_lookup = (np.nanargmin, np.nanmin) \
        if args.metric in ['loss'] else (np.nanargmax, np.nanmax)
    key = get_key(args.train_metric)
    model_dir_no_lr = create_model_dir(args, lr=False)
    lr_dirs = [lr_dir for lr_dir in os.listdir(model_dir_no_lr)
               if os.path.isdir(os.path.join(model_dir_no_lr, lr_dir))
               and not lr_dir.startswith('.')]
    runs_metric = list()
    for lr_dir in lr_dirs:
        # /*/ for different seeds
        lr_metric_dirs = glob.glob(
            model_dir_no_lr + '/' + lr_dir + '/*/full_metrics.json')
        if len(lr_metric_dirs) == 0:
            runs_metric.append(np.nan)
        else:
            lr_metric = list()
            for lr_metric_dir in lr_metric_dirs:
                with open(lr_metric_dir) as json_file:
                    metrics = json.load(json_file)
                metric_values = metrics[key + args.metric]
                metric = metric_values[-1] if last else \
                    best_lookup(metric_values)
                lr_metric.append(metric)
            runs_metric.append(np.mean(lr_metric))

    i_best_lr = best_arg(runs_metric)
    best_metric = runs_metric[i_best_lr]
    best_lr = lr_dirs[i_best_lr]
    return best_lr, best_metric, lr_dirs


def get_best_runs(args_exp):
    model_dir_no_lr = create_model_dir(args_exp, lr=False)
    best_lr, _, _ = get_best_lr_and_metric(args_exp, last=False)
    model_dir_lr = os.path.join(model_dir_no_lr, best_lr)
    json_dir = 'best_metrics.json'
    metric_dirs = glob.glob(model_dir_lr + '/*/' + json_dir)

    print(f'Method: {args_exp.model} best_params: {best_lr}')
    with open(metric_dirs[0]) as json_file:
        metric = json.load(json_file)
    runs = [metric]

    for metric_dir in metric_dirs[1:]:
        with open(metric_dir) as json_file:
            metric = json.load(json_file)
        # ignores failed runs
        if not np.isnan(metric['loss']).any():
            runs.append(metric)
    return runs


def get_train_inputs(data, label, model, batch_size, device, is_rnn):
    if not is_rnn:
        input = (data,)
    else:
        hidden = model.init_hidden(batch_size, device)
        input = (data, hidden)
        # label = label.reshape(-1)
    return input, label



def forward_backward(model, criterion, input, label, inference_duration,
                     backprop_duration, batch_size, metrics_meter, is_rnn,
                     prox, lam):
    
    
    outputs = model(*input)

    if not is_rnn:
        hidden = None
        output = outputs
    else:
        output, hidden = outputs

    
    

    loss = compute_loss(criterion, output, label)
    calc_loss = loss
    if not prox:
        loss += model.regularize(lam=lam)
    loss.backward()
   

    
    return inference_duration, backprop_duration, output, hidden,calc_loss


def forward_test(model, criterion, input, label,batch_size, is_rnn,
                     prox, lam):
    
    
    outputs = model(*input)

    if not is_rnn:
        hidden = None
        output = outputs
    else:
        output, hidden = outputs

    
    

    loss = compute_loss(criterion, output, label)
    calc_loss = loss.item()

    return calc_loss



def compute_loss(criterion, output, label):
    loss = criterion(output.float(), label.float())
    return loss


def update_metrics(metrics_meter, loss, batch_size):
    metrics_meter['loss'].update(loss.item(), batch_size)


def get_optimiser(params_to_update, optimiser_name, lr, momentum, weight_decay):
    if optimiser_name == 'sgd':
        optimiser = SGD(params_to_update, lr,
                        momentum=momentum,
                        weight_decay=weight_decay)
    elif optimiser_name == 'adam':
        optimiser = Adam(params_to_update, lr)
    elif optimiser_name == 'rmsprop':
        optimiser = RMSprop(params_to_update, lr,
                            momentum=momentum,
                            weight_decay=weight_decay)
    else:
        raise ValueError("optimiser not supported")

    return optimiser


def run_one_epoch(
        model, train_loader, criterion, optimiser,
        device, epochs, is_rnn=False, clip_grad=False, prox=False, lam=0.,
        print_freq=10,seq_len=None,inp_len=None):

    metrics_meter = init_metrics_meter(epochs)
    model.train()

    batch_loss = 0
    batch_size = 0


    for data_x, data_y in train_loader:
        batch_size += 1
        start_ts = time.time()
        batch_size = data_x.shape[0]
        print(batch_size)
        data_x = data_x.reshape(-1,seq_len,inp_len).to(device)
        print('X_data Shape')
        print(data_x.shape)
        data_y = data_y.to(device)
        dataload_duration = time.time() - start_ts

        inference_duration = 0.
        backprop_duration = 0.

        optimiser.zero_grad(set_to_none=True)
        input= data_x
        label = data_y

        input, label = get_train_inputs(
            input, label, model, batch_size, device, is_rnn)

        inference_duration, backprop_duration, _, _ ,loss= \
            forward_backward(
                model, criterion, input, label, inference_duration,
                backprop_duration, batch_size, metrics_meter, is_rnn,
                prox, lam)
        #if is_rnn or clip_grad:
         #   nn.utils.clip_grad_norm_(model.parameters(), CLIP_RNN_GRAD)
        optimiser.step()
        optimiser.zero_grad(set_to_none=True)
        batch_loss += loss.item()



    return metrics_meter,(batch_loss/batch_size)



def test_on_test_data(
        model, train_loader, criterion, optimiser,
        device, epochs, is_rnn=False, clip_grad=False, prox=False, lam=0.,
        print_freq=10,seq_len=None,inp_size=None):

   

    batch_loss = 0
    bs = 0

    for data_x, data_y in train_loader:
        bs += 1

        batch_size = data_x.shape[0]


        data_x = data_x.reshape(-1,seq_len,inp_size)


        
        

        

        
        input= data_x.float()
        label = data_y

 

        input, label = get_train_inputs(
           input, label, model, batch_size, device, is_rnn)
        input[0].float()
        #input = input.float()
   
        test_loss= forward_test(
                model, criterion, input, label, batch_size, is_rnn,
                prox, lam)
        #if is_rnn or clip_grad:
         #   nn.utils.clip_grad_norm_(model.parameters(), CLIP_RNN_GRAD)

        batch_loss += test_loss


    return (batch_loss/bs)










def run_one_epoch_filter(
        model, train_loader, criterion, optimiser,
        device, epochs, is_rnn=False, clip_grad=False, prox=False, lam=0.,
        print_freq=10,seq_len=None,inp_size=None):

    metrics_meter = init_metrics_meter(epochs)
    model.train()
    batch_loss = 0
    batch_size = 0

    for data_x, data_y in train_loader:
        batch_size += 1
        start_ts = time.time()
        batch_size = data_x.shape[0]


        data_x = data_x.reshape(-1,seq_len,inp_size)


        data_y = data_y.to(device)
        dataload_duration = time.time() - start_ts

        inference_duration = 0.
        backprop_duration = 0.

        optimiser.zero_grad(set_to_none=True)
        input= data_x.float()
        label = data_y

 

        input, label = get_train_inputs(
           input, label, model, batch_size, device, is_rnn)
        input[0].float()
        #input = input.float()
   
        inference_duration, backprop_duration, _, _ ,loss= \
            forward_backward(
                model, criterion, input, label, inference_duration,
                backprop_duration, batch_size, metrics_meter, is_rnn,
                prox, lam)
        #if is_rnn or clip_grad:
         #   nn.utils.clip_grad_norm_(model.parameters(), CLIP_RNN_GRAD)
        optimiser.step()
        optimiser.zero_grad(set_to_none=True)
        batch_loss += loss


    return metrics_meter,(batch_loss/batch_size)


def init_and_train_model(model,optimiser,lr,momentum,weight_decay, train_loader,test_loader,total_epochs,method='static',lam = 0.001,prox=True,seq_len=5,inp_size=10):
    #full_metrics = init_metrics_meter()
    #model_dir = create_model_dir(args)
    # don't train if setup already exists
    #if os.path.isdir(model_dir):
        #Logger.get().info(f"{model_dir} already exists.")
        #Logger.get().info("Skipping this setup.")
        #return
    # create model directory
    #os.makedirs(model_dir, exist_ok=True)
    # init wandb tracking
    # wandb.init(
    #     project="insert_project_name", entity="insert_entity", config=vars(args),
    #     name=str(create_model_dir(args, lr=False)), reinit=True)
    # save used args as json to experiment directory
    #with open(os.path.join(create_model_dir(args), 'args.json'), 'w') as f:
        #json.dump(vars(args), f, indent=4)
    option = model
    if model == 'cmlpwf':
      model = cmlpwf(input_size=inp_size, seq_len=seq_len).cuda(device=device)
      #model = LeKVAR(inp ,seq_len,hidden_dim =10,n_layers =2,mode ='nn').cpu()
    if model == 'cmlp':
      model = cmlp(input_size=inp_size, seq_len=seq_len).cuda(device=device)
  
    criterion = nn.MSELoss()

    

    optimiser = get_optimiser(
        model.parameters(), optimiser, lr,
        momentum, weight_decay)

    scheduler = get_lr_scheduler(
        optimiser, total_epochs, method)

    #metric_to_optim = loss
    best_metric = np.inf
    train_time_meter = 0
    best_model = None
    test_loss_list = []
    train_loss_list = []

    for i in range(total_epochs):
      if option == 'cmlpwf':

      
        metrics_meter,loss = run_one_epoch_filter(
            model, train_loader, criterion, optimiser, device,
            1, is_rnn=False, clip_grad=False, lam = lam,prox=True,inp_size=inp_size,seq_len=seq_len)
        if prox:

          prox_step(model=model,scheduler=scheduler , lam=lam)
        train_loss_list.append(loss.item())

        # wandb.log(metric_to_dict(metrics_meter, i+1, 'train', False))
    
        
        print('Epoch : ',i,':----',' Train Loss : ',loss.item())
        test_loss = test_on_test_data(
            model, test_loader, criterion, optimiser, device,
            1, is_rnn=False, clip_grad=False, lam = lam,prox=True,inp_size=inp_size,seq_len=seq_len)
        print('Epoch : ',i,':----',' Test Loss : ',test_loss)
        test_loss_list.append(test_loss)
        # Track timings across epochs
        if test_loss < best_metric:
          best_model = model
          best_metric = test_loss





        scheduler.step()
        i += 1
        torch.cuda.empty_cache()
 


    return best_model,loss,best_metric

"""Epoch and Preprocess the Data"""

def epoch(X,seq_len):
  D=[]
  Y = []
  for i in range(0,len(X)):
    if i +seq_len < len(X):
      x = X[i:i+seq_len,:]
      y = X[i+seq_len]
      D.append(x)
      Y.append(y)
  return np.array(D,dtype=np.float),np.array(Y,dtype=np.float)

from sklearn.preprocessing import StandardScaler


Epilepsy_data = pd.read_csv('/content/drive/MyDrive/Epilepsy_Data.csv')
Epilepsy_data = Epilepsy_data.iloc[:,1:]
Epilepsy_data.shape

all_data = []
overlap = 1000;
for ind in range(0,Epilepsy_data.shape[0],1000):
  all_data.append(Epilepsy_data.iloc[ind:ind+2000,:])



all_X = []
all_Y = []
for d in all_data:
  X,Y = epoch(np.array(d),3)
  X = X.reshape(X.shape[0],-1)

  all_X.append(X)
  all_Y.append(Y)

all_Y[0].shape

scaled_X_train = []
scaled_X_test = []
scaled_Y_train = []
scaled_Y_test = []
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
for i in range(0,len(all_X)):
  X_train,X_test,Y_train,Y_test = train_test_split(all_X[i],all_Y[i],test_size=0.25)
  sc = StandardScaler()
  sc = sc.fit(X_train)
  X_train = sc.transform(X_train)
  X_test = sc.transform(X_test)
  sc = StandardScaler()
  sc = sc.fit(Y_train)
  Y_train = sc.transform(Y_train)
  Y_test = sc.transform(Y_test)
  ###############################
  X_train = X_train.reshape(-1,3,19)
  X_test  = X_test.reshape(-1,3,19)
  #################################
  scaled_X_train.append(X_train)
  scaled_X_test.append(X_test)
  scaled_Y_train.append(Y_train)
  scaled_Y_test.append(Y_test)

scaled_Y_train[0].shape

"""Scale the Data """

from torch.utils.data import Dataset,DataLoader
#X_train = torch.tensor(X_train, dtype=torch.float32, device=device)
#Y_train = torch.tensor(Y_train, dtype=torch.float32, device=device)
train_data_sets = []
test_data_sets = []
for i in range(0,len(scaled_X_train)):

  x_train = torch.tensor(scaled_X_train[i], dtype=torch.float64, device=device)
  y_train = torch.tensor(scaled_Y_train[i], dtype=torch.float64, device=device)
  ds = PrepareData(X=x_train, y=y_train, scale_X=False)
  train_loader = DataLoader(ds,batch_size=64,shuffle = True)
  train_data_sets.append(train_loader)

  x_test = torch.tensor(scaled_X_test[i], dtype=torch.float64, device=device)
  y_test = torch.tensor(scaled_Y_test[i], dtype=torch.float64, device=device)
  
  ds_test = PrepareData(X=x_test, y=y_test, scale_X=False)
  test_loader = DataLoader(ds_test,batch_size=64,shuffle = True)
  test_data_sets.append(test_loader)

print(next(iter(test_loader))[0].shape)

# BS, Seq, Features

def scale(X):
  array = np.zeros(shape = X.shape)
  for i in range(0,X.shape[0]):
    x = X[i]
    maxa =  max(x)
    mina = min(x)
    for j in range(0,X.shape[1]):
      array[i,j] = (X[i,j]-mina)/(maxa-mina);
  return array

main_dir = '/content/drive/MyDrive/SpectralDataAnalysisCMLPWF'
#os.mkdir('/content/drive/MyDrive/SpectralDataAnalysisCMLPWF')
all_models = []
all_losses = []
all_train_losses = []
gc_total_arrays =[]
for datas in range(1,len(scaled_X_train)):

  
  models = []
  losses = []
  train_losses = []
  lr_ = [0.001,0.01]
  lamd = [1e-5,1e-4,1e-3,1e-2]
  gc = np.zeros(shape = (19,19))
  ## to save models 
  models_dir_name = 'Models' + str('Data') + str(datas)
  models_dir_path = os.path.join(main_dir,models_dir_name)
  os.mkdir(models_dir_path)

 


  for learning in lr_:
    for lam in lamd:
      model,train_loss,test_loss=init_and_train_model(model='cmlpwf',optimiser='adam',lr=learning,momentum=0.99,weight_decay=None, train_loader=train_data_sets[datas],test_loader=test_data_sets[datas],total_epochs=200,method='static',lam = lam,prox=False,seq_len=3,inp_size=19)
      models.append(model)
      losses.append(test_loss)
      train_losses.append(train_loss.detach().cpu().numpy())

      gc_sc = model.GC(ignore_lag = True)
      gc_sc = scale(gc_sc)
      gc_sc = (gc_sc >= 0.5).astype(int)
      gc += gc_sc
      ##### Saving Models
      model_specific = str('Learning_rate')+str(learning)+str('Lamda') + str(lam) +'.pth'
      ind_model_path = os.path.join(models_dir_path,model_specific)
      #os.mkdir(ind_model_path)
      torch.save(model,ind_model_path)
      
  gc_total_arrays.append(gc)
  all_models.append(models)
  all_losses.append(losses)
  all_train_losses.append(train_losses)
######## Saving GC
  gc_total_dir_name = str('Data')+str(datas) +str('GC_total')
  gc_total_path = os.path.join(main_dir , gc_total_dir_name)
  os.mkdir(gc_total_path)
  gc_total_path = os.path.join(gc_total_path,'.csv')
  df = pd.DataFrame(gc)
  df.to_csv(gc_total_path)
######## Saving Testing Losses 

  test_loss_total_dir_name = str('Data')+str(datas) +str('testloss')
  test_loss_total_path = os.path.join(main_dir , test_loss_total_dir_name)
  test_losss = np.array(all_losses)
  os.mkdir(test_loss_total_path)
  test_loss_total_path = os.path.join(test_loss_total_path,'.csv')
  
  test_losss.tofile(test_loss_total_path, sep = ',')
######## Saving Training Losses 
  train_loss_total_dir_name = str('Data')+str(datas) +str('trainloss')
  train_loss_total_path = os.path.join(main_dir , train_loss_total_dir_name)
  train_losss = np.array(all_train_losses)
  os.mkdir(train_loss_total_path)

  train_loss_total_path = os.path.join(train_loss_total_path,'.csv')

  train_losss.tofile(train_loss_total_path, sep = ',')

gc_total_arrays[0]

import pandas as pd
df = pd.DataFrame(gc_total_arrays[0])
df.to_csv()

import seaborn as sns
sns.heatmap(GC_1,annot=True)

sns.heatmap(all_models[0][-1].GC(ignore_lag=True),annot=True)

import pandas as pd
data=pd.DataFrame({"Train_loss":train_losses,"Test Losses":losses})
data['Diff'] = np.abs(data.iloc[:,0] - data.iloc[:,1])
data

len(all_models)

gcs = []
for i in models:
  gc_sc = scale(i.GC(ignore_lag=True))
  gc_sc = (gc_sc >= 0.5).astype(int)
  gcs.append(gc_sc)
gc_total = np.zeros(shape = gcs[0].shape)
for i in gcs:
  gc_total += i

gc_total

gc_total_s = (gc_total>2).astype(int)

model = models[-1]

GC_test[2]

gc_total

model = models[-2]
GC = model.GC(ignore_lag=True)
GC_sc=scale(GC)
GC_sc=GC_sc >0.5
GC_sc.astype(int)
import matplotlib.pyplot as plt
import seaborn as sns
GC_actual = GC_1
plt.figure(figsize=(8,8))
f,(ax1,ax2)= plt.subplots(1,2,figsize=(8,4))
ax1.set_title('Actual')
sns.heatmap(GC_actual,ax=ax1,annot=True)
ax2.set_title('Predicted')
#sns.heatmap(gc_total_s,ax = ax2,annot=True)
sns.heatmap(GC_sc,ax = ax2,annot=True)
considered = GC_sc
GC_actual = GC_1
# Mark disagreements
count=0
for i in range(10):
    for j in range(10):
        if GC_actual[i, j] != considered[i, j]:
            count+=1
            rect = plt.Rectangle((j, i-0.05), 1, 1, facecolor='none', edgecolor='red', linewidth=1)
            ax2.add_patch(rect)

plt.show()
(-(count/100)+1)*100

import matplotlib.pyplot as plt
for i in range(0,models[12].GC(ignore_lag = True).shape[0]):
  plt.figure(figsize=(10,10))
  plt.bar(np.arange(1,model.GC(ignore_lag=True).shape[0]+1),models[12].GC(ignore_lag=True)[i])
  plt.show()

GC





GC =model.GC(ignore_lag=True)
GC = softmax(GC,axis=1)
plt.figure(figsize=(10,10))
plt.bar(np.arange(0,len(GC[0])),GC[0])
plt.xlim(0,19)
plt.show()
GC[0]

import matplotlib.pyplot as plt
import seaborn as sns
GC = model.GC(ignore_lag = True)
for j,i in enumerate(GC):
  plt.figure(figsize=(10,10))
  plt.title(j)
  sns.barplot(np.arange(0,len(i)),i)
  plt.show()



import numpy as np
from scipy.integrate import odeint


def make_var_stationary(beta, radius=0.97):
    """Rescale coefficients of VAR model to make it stable."""
    p = beta.shape[0]
    lag = beta.shape[1] // p
    bottom = np.hstack((np.eye(p * (lag - 1)), np.zeros((p * (lag - 1), p))))
    beta_tilde = np.vstack((beta, bottom))
    eigvals = np.linalg.eigvals(beta_tilde)
    max_eig = max(np.abs(eigvals))
    nonstationary = max_eig > radius
    if nonstationary:
        return make_var_stationary(0.95 * beta, radius)
    else:
        return beta


def simulate_var(p, T, lag, sparsity=0.2, beta_value=1.0, sd=0.1, seed=0, delay=0, kernel=None,
                 GC=None, beta=None):
    if seed is not None:
        np.random.seed(seed)

    # Set up coefficients and Granger causality ground truth.
    if GC is None:
        GC = [np.eye(p, dtype=int) for _ in range(lag)]
        beta = [np.eye(p) * beta_value for _ in range(lag)]

        num_nonzero = int(p * sparsity) - 1
        for i in range(p):
            choice = np.random.choice(p - 1, size=num_nonzero, replace=False)
            choice[choice >= i] += 1
            for l in range(lag):
                beta[l][i, choice] = beta_value
                GC[l][i, choice] = 1

        beta = np.hstack(beta)
        beta = make_var_stationary(beta)

        GC = np.array(GC)
    else:
        assert beta is not None, 'beta has to be provided.'

    # Generate data.
    burn_in = 100
    errors = np.random.normal(scale=sd, size=(p, T + burn_in))
    X = np.zeros((p, T + burn_in))
    X[:, :lag] = errors[:, :lag]
    for t in range(lag + delay, T + burn_in):
        val = X[:, (t-lag - delay):t - delay]
        if kernel is not None:
            val = np.apply_along_axis(kernel, 0, val)
        X[:, t] = np.dot(beta, val.flatten(order='F'))
        X[:, t] += + errors[:, t-1]

    return X.T[burn_in:], beta, GC


def lorenz(x, t, F):
    """Partial derivatives for Lorenz-96 ODE."""
    p = len(x)
    dxdt = np.zeros(p)
    for i in range(p):
        dxdt[i] = (x[(i+1) % p] - x[(i-2) % p]) * x[(i-1) % p] - x[i] + F

    return dxdt


def simulate_lorenz_96(p, T, F=10.0, delta_t=0.1, sd=0.1, burn_in=1000,
                       seed=0):
    if seed is not None:
        np.random.seed(seed)

    # Use scipy to solve ODE.
    x0 = np.random.normal(scale=0.01, size=p)
    t = np.linspace(0, (T + burn_in) * delta_t, T + burn_in)
    X = odeint(lorenz, x0, t, args=(F,))
    X += np.random.normal(scale=sd, size=(T + burn_in, p))

    # Set up Granger causality ground truth.
    GC = np.zeros((p, p), dtype=int)
    for i in range(p):
        GC[i, i] = 1
        GC[i, (i + 1) % p] = 1
        GC[i, (i - 1) % p] = 1
        GC[i, (i - 2) % p] = 1

    return X[burn_in:], GC

p = 10
T = 1000
delay = 0
lag = 3
# kernel = lambda x: - 2 + 1/2 * x**2
# k = 'sq'
kernel = None
k = 'none'


X_np, beta, GC = simulate_var(p=p, T=T, lag=lag, delay=delay, kernel=kernel, seed=100)

    
#X_np, beta, GC = simulate_var(p=p, T=T, lag=lag, delay=delay, kernel=kernel, seed=100, GC=GC, beta=beta)

GC_actual = GC[0]
GC_actual

X,Y = epoch(X_np,3)

device = torch.device('cuda')
X = torch.tensor(X, dtype=torch.float32, device=device)
Y = torch.tensor(Y, dtype=torch.float32, device=device)

seq_len =3;
channels =10 ;
X = X.reshape(-1,seq_len*channels)

ds = PrepareData(X, y=Y, scale_X=False)

train_set = DataLoader(ds, batch_size=64)

for i,j in train_set:
  print(i.shape)
  print(j.shape)


model=init_and_train_model(optimiser='adam',lr=0.01,momentum=0.99,weight_decay=0.05, trainloader=train_set,total_epochs=200,method='cosine',lam = 0.01,prox=False,input_size=10,seq_len=3)

GC=model.GC(ignore_lag =True)

GC_sc=scale(GC)
GC_sc=GC_sc > 0.5
GC_sc.astype(int)

plt.figure(figsize=(8,8))
f,(ax1,ax2)= plt.subplots(1,2,figsize=(8,4))
ax1.set_title('Actual')
sns.heatmap(GC_actual,ax=ax1,annot=True)
ax2.set_title('Predicted')
sns.heatmap(GC_sc,ax = ax2,annot=True)

from scipy.special import softmax
GC = softmax(model.GC(ignore_lag=True),axis=1)
GC = GC>0.5
GC.astype(int)


plt.figure(figsize=(8,8))
f,(ax1,ax2)= plt.subplots(1,2,figsize=(8,4))
ax1.set_title('Actual')
sns.heatmap(GC_actual,ax=ax1,annot=True)
ax2.set_title('Predicted')
sns.heatmap(GC,ax = ax2,annot=True)

import seaborn as sns
for i in model.GC(ignore_lag =False)[0]:
  print(i)
  plt.figure(figsize=(10,10))
  sns.barplot(np.arange(0,len(i)),i)
  plt.show()

GC[1]

from torchinfo import summary
summary(model, input_size=(64, 190))
inp=np.random.rand(64,190)
inp.reshape((*inp.shape,1)).shape

(list(model.parameters()))[-2]>0

for name, param in model.named_parameters():
  print(name)
  print(param)



pred=model(X)

Y = next(iter(train_set))[1]

import matplotlib.pyplot as plt
plt.scatter(pred[:,18].cpu().detach(),Y[:,18].cpu().detach())

criterion = nn.MSELoss()

from torch.utils.data import Dataset,DataLoader


def train_model(model,dataset_loader,max_iter=100,lam=0,lr=0.0001):
  optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
  total_loss = []
  best_model = None
  best_loss = np.inf
  print(max_iter)
  for epoch in range(1,max_iter):
    loss = 0
    c =0


    for X,Y in dataset_loader:
   
      loss = criterion(model(X),Y) + model.regularize(lam)
      loss.backward()
      model.prox(lam)
      optimizer.step()
      
      optimizer.zero_grad()
      
      loss += loss
      c += 1
      total_loss.append(loss/c)
    print('-----',"Iteration : ",epoch,' : ','Loss : ',total_loss[epoch])
    print(model.GC().shape)
    
    if loss < best_loss:
      best_loss = loss
      best_model = model
  return best_model

bm=train_loss_list = train_model(
    lekvar, train_set, lam=0.0080,max_iter =10)

